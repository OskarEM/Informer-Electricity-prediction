{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IM6CZzW_CH0"
   },
   "source": [
    "# Informer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdaIHYx4_ECL"
   },
   "source": [
    "## Download code and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SA_i2gbl-rn-",
    "outputId": "41dadbf7-6be2-423c-ed51-1e321f42be68"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/zhouhaoyi/ETDataset.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5GFng7v7Eq0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if not 'Informer2020' in sys.path:\n",
    "    sys.path += ['Informer2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YW9TS6jp_YXc"
   },
   "outputs": [],
   "source": [
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('./part4.csv')\n",
    "\n",
    "# Exclude columns that start with 'lat_' or 'lon_'\n",
    "data = data[data.columns.drop(list(data.filter(regex='lat_|lon_')))]\n",
    "\n",
    "# Save the modified dataset\n",
    "data.to_csv('./Data2023.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIjZdN5e_SWe"
   },
   "source": [
    "## Experiments: Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RPdt-Kwc_RRZ"
   },
   "outputs": [],
   "source": [
    "from utils.tools import dotdict\n",
    "from exp.exp_informer import Exp_Informer\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "import gc\n",
    "os.environ['WANDB_AGENT_DISABLE_FLAPPING'] = 'true'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6mx2dnwY9dWi"
   },
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
    "\n",
    "args.data = 'custom' # data\n",
    "args.root_path = './' # root path of data file\n",
    "args.data_path = 'Data.csv' # data file\n",
    "args.features = 'MS' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
    "args.target = 'south' # target feature in S or MS task\n",
    "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
    "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
    "\n",
    "args.seq_len = 45 # input sequence length of Informer encoder\n",
    "args.label_len = 3  # start token length of Informer decoder\n",
    "args.pred_len = 1 # prediction sequence length\n",
    "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
    "\n",
    "args.enc_in = 32 # encoder input size 7\n",
    "args.dec_in = 32 # decoder input size 7\n",
    "args.c_out = 1 # output size 7\n",
    "args.factor = 4 # probsparse attn factor was 5 - 7 \n",
    "args.d_model = 1078 # dimension of model\n",
    "args.n_heads = 11 # num of heads 8\n",
    "args.e_layers = 4 # num of encoder layers 2\n",
    "args.d_layers = 2 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model 2048\n",
    "args.dropout = 0.25 # dropout 0.05, best for me 0.1\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = True # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'h'\n",
    "\n",
    "args.batch_size = 76\n",
    "args.learning_rate = 0.0001 #191\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False # whether to use automatic mixed precision training\n",
    "\n",
    "args.inverse = True\n",
    "args.scale = True\n",
    "\n",
    "args.num_workers = 0\n",
    "args.itr = 1\n",
    "args.train_epochs = 100\n",
    "args.patience = 100\n",
    "args.des = 'exp'\n",
    "\n",
    "args.use_gpu = True if torch.cuda.is_available() else False\n",
    "args.gpu = 0\n",
    "\n",
    "args.use_multi_gpu = False\n",
    "args.devices = '0,1,2,3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k_BCYODAwKl9"
   },
   "outputs": [],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.devices = args.devices.replace(' ','')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "53o3pZ809p-a"
   },
   "outputs": [],
   "source": [
    "# Set augments by using data name\n",
    "data_parser = {\n",
    "    'custom':{'data':'Data.csv','T':'Avg','M':[34 ,34 ,34 ],'S':[1,1,1],'MS':[34 ,34 ,1]},\n",
    "  \n",
    "}\n",
    "if args.data in data_parser.keys():\n",
    "    data_info = data_parser[args.data]\n",
    "    args.data_path = data_info['data']\n",
    "    args.target = data_info['T']\n",
    "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yZ5Q2vyKwSfk"
   },
   "outputs": [],
   "source": [
    "args.detail_freq = args.freq\n",
    "args.freq = args.freq[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ywY-umrw-mHO",
    "outputId": "9b2e2e8e-7025-46e6-e2d7-2d8c9ceeb381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'model': 'informer', 'data': 'custom', 'root_path': './', 'data_path': 'Data.csv', 'features': 'MS', 'target': 'Avg', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 45, 'label_len': 3, 'pred_len': 1, 'enc_in': 34, 'dec_in': 34, 'c_out': 1, 'factor': 4, 'd_model': 1078, 'n_heads': 11, 'e_layers': 4, 'd_layers': 2, 'd_ff': 2048, 'dropout': 0.25, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': True, 'mix': True, 'padding': 0, 'batch_size': 76, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'inverse': True, 'scale': True, 'num_workers': 0, 'itr': 1, 'train_epochs': 100, 'patience': 100, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
     ]
    }
   ],
   "source": [
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KVHZhRB4-on9"
   },
   "outputs": [],
   "source": [
    "Exp = Exp_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "sweep_config = {\n",
    "    'method': 'bayes',  # Bayesian optimization\n",
    "    'metric': {\n",
    "        'name': 'RMSE',  # Metric to optimize\n",
    "        'goal': 'minimize'    # Aim to minimize validation loss\n",
    "    },\n",
    "    'parameters': {\n",
    "        'batch_size': {\n",
    "              'min' : 8,\n",
    "            'max' : 64,\n",
    "            #'values': [   8, 16, 24, 36] #8\n",
    "        },\n",
    "        'n_heads': {\n",
    "            # 'min' : 5,\n",
    "            #'max' : 17,\n",
    "             'values': [  8, 9, 10, 11, 12, 13, 14, 15 , 16 ] #14\n",
    "        },\n",
    "        'factor': {\n",
    "             'min' : 3,\n",
    "            'max' : 9,\n",
    "            #'values': [  5, 7, 9, 11, 13, 15 , 17, 19 ,21, 22] #7\n",
    "        },\n",
    "        'd_model': {\n",
    "             #'min' : 278,\n",
    "            #'max' : 878,\n",
    "            'values': [  678, 778, 878, 978, 1078, 1178, 1278, 1378,\n",
    "                       1478, 1578] #1500 #1078\n",
    "        },\n",
    "        'd_ff' : { #2048\n",
    "             'min' : 1648,\n",
    "            'max' : 2648,\n",
    "            #'values': [ 848, 1048, 1248, 1448, 1648, 1848, 2048, 2248, 2448, 2648]\n",
    "        },\n",
    "         'seq_len': {\n",
    "            'values': [24*3*3, 12*3*3]\n",
    "        },\n",
    "        'label_len': {\n",
    "            'values': [24*3, 12*3]\n",
    "        },\n",
    "        'pred_len': {\n",
    "            'values': [24, 12]\n",
    "        }\n",
    "        # You can add other parameters here that you want to optimize\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    with wandb.init() as run:\n",
    "        # Update args with the sweep parameters\n",
    "        args.n_heads = run.config.n_heads\n",
    "        args.batch_size = run.config.batch_size\n",
    "        args.d_model = run.config.d_model\n",
    "        args.factor = run.config.factor\n",
    "\n",
    "        # Update other args parameters if needed\n",
    "\n",
    "        wandb.config.update(args)\n",
    "\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}12'.format(\n",
    "                args.model, args.data, args.features, args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, \n",
    "                args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "            # set experiments\n",
    "            exp = Exp(args)\n",
    "\n",
    "            # train\n",
    "            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "            exp.train(setting)\n",
    "\n",
    "            # test\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "            if args.do_predict:\n",
    "                print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.predict(setting, True)\n",
    "            gc.collect()\n",
    "        run.finish()\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "# Initialize the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='Elprices', entity='ossyandlars')\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "928tzaA2AA2g",
    "outputId": "c19f673a-02d1-4f4d-91c3-d0f25e600443"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/oskarem/DNNPROJECT/Informer2020-main/wandb/run-20231207_164748-dlw7nj0c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ossyandlars/ELprices/runs/dlw7nj0c' target=\"_blank\">winter-paper-2502</a></strong> to <a href='https://wandb.ai/ossyandlars/ELprices' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ossyandlars/ELprices' target=\"_blank\">https://wandb.ai/ossyandlars/ELprices</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ossyandlars/ELprices/runs/dlw7nj0c' target=\"_blank\">https://wandb.ai/ossyandlars/ELprices/runs/dlw7nj0c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : informer_custom_ftMS_sl45_ll3_pl1_dm1078_nh11_el4_dl2_df2048_atprob_fc4_ebtimeF_dtTrue_mxTrue_exp_0relu2021_2023_1HE100P3>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 7010\n",
      "val 393\n",
      "test 391\n",
      "Epoch: 1 cost time: 21.177915573120117\n",
      "Epoch: 1, Steps: 92 | Train Loss: 863.2930183 Vali Loss: 287.8845551 Test Loss: 202.1205246\n",
      "Validation loss decreased (inf --> 287.884555).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 cost time: 21.381032705307007\n",
      "Epoch: 2, Steps: 92 | Train Loss: 347.7395459 Vali Loss: 248.7390961 Test Loss: 200.7934998\n",
      "Validation loss decreased (287.884555 --> 248.739096).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 cost time: 20.975331783294678\n",
      "Epoch: 3, Steps: 92 | Train Loss: 261.0310777 Vali Loss: 264.8907227 Test Loss: 176.0059174\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 cost time: 20.667306900024414\n",
      "Epoch: 4, Steps: 92 | Train Loss: 240.5367307 Vali Loss: 232.7378693 Test Loss: 179.1194504\n",
      "Validation loss decreased (248.739096 --> 232.737869).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 cost time: 20.458329916000366\n",
      "Epoch: 5, Steps: 92 | Train Loss: 221.5049370 Vali Loss: 236.5797363 Test Loss: 190.4089676\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 cost time: 21.174235820770264\n",
      "Epoch: 6, Steps: 92 | Train Loss: 212.6179538 Vali Loss: 243.5934296 Test Loss: 184.7142883\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 cost time: 20.87861680984497\n",
      "Epoch: 7, Steps: 92 | Train Loss: 207.2011487 Vali Loss: 266.5253906 Test Loss: 195.0110428\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 cost time: 20.773061513900757\n",
      "Epoch: 8, Steps: 92 | Train Loss: 204.3848583 Vali Loss: 255.7188446 Test Loss: 190.2403870\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 cost time: 21.080036640167236\n",
      "Epoch: 9, Steps: 92 | Train Loss: 201.6160389 Vali Loss: 251.5136383 Test Loss: 192.3981827\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 cost time: 20.868902683258057\n",
      "Epoch: 10, Steps: 92 | Train Loss: 206.6853422 Vali Loss: 250.9865189 Test Loss: 196.3237350\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 1.953125e-07\n",
      "Epoch: 11 cost time: 20.974676847457886\n",
      "Epoch: 11, Steps: 92 | Train Loss: 204.8543904 Vali Loss: 242.5297440 Test Loss: 188.2164017\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 9.765625e-08\n",
      "Epoch: 12 cost time: 20.789679050445557\n",
      "Epoch: 12, Steps: 92 | Train Loss: 206.0653445 Vali Loss: 233.7901001 Test Loss: 187.4721054\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 4.8828125e-08\n",
      "Epoch: 13 cost time: 23.094019651412964\n",
      "Epoch: 13, Steps: 92 | Train Loss: 210.2099429 Vali Loss: 244.4107239 Test Loss: 184.2896591\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 2.44140625e-08\n",
      "Epoch: 14 cost time: 21.560092210769653\n",
      "Epoch: 14, Steps: 92 | Train Loss: 205.5803755 Vali Loss: 251.1784851 Test Loss: 184.0594376\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 1.220703125e-08\n",
      "Epoch: 15 cost time: 22.256786346435547\n",
      "Epoch: 15, Steps: 92 | Train Loss: 202.5921822 Vali Loss: 248.6600159 Test Loss: 186.0614700\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 6.103515625e-09\n",
      "Epoch: 16 cost time: 21.747416257858276\n",
      "Epoch: 16, Steps: 92 | Train Loss: 204.4797422 Vali Loss: 251.4313873 Test Loss: 188.3150696\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 3.0517578125e-09\n",
      "Epoch: 17 cost time: 22.415712594985962\n",
      "Epoch: 17, Steps: 92 | Train Loss: 199.1740225 Vali Loss: 250.5098419 Test Loss: 186.5120636\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Updating learning rate to 1.52587890625e-09\n",
      "Epoch: 18 cost time: 22.952967643737793\n",
      "Epoch: 18, Steps: 92 | Train Loss: 204.5909904 Vali Loss: 249.3292023 Test Loss: 189.1563354\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Updating learning rate to 7.62939453125e-10\n",
      "Epoch: 19 cost time: 22.85461163520813\n",
      "Epoch: 19, Steps: 92 | Train Loss: 206.1805619 Vali Loss: 242.3558289 Test Loss: 184.3716324\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Updating learning rate to 3.814697265625e-10\n",
      "Epoch: 20 cost time: 21.571216344833374\n",
      "Epoch: 20, Steps: 92 | Train Loss: 205.9309089 Vali Loss: 243.5185852 Test Loss: 181.6572144\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Updating learning rate to 1.9073486328125e-10\n",
      "Epoch: 21 cost time: 21.168112754821777\n",
      "Epoch: 21, Steps: 92 | Train Loss: 206.8090475 Vali Loss: 247.9474762 Test Loss: 184.3160019\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Updating learning rate to 9.5367431640625e-11\n",
      "Epoch: 22 cost time: 22.266068935394287\n",
      "Epoch: 22, Steps: 92 | Train Loss: 204.0615511 Vali Loss: 240.8807495 Test Loss: 184.1631012\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Updating learning rate to 4.76837158203125e-11\n",
      "Epoch: 23 cost time: 22.564316987991333\n",
      "Epoch: 23, Steps: 92 | Train Loss: 203.2860270 Vali Loss: 249.8999603 Test Loss: 185.2624435\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Updating learning rate to 2.384185791015625e-11\n",
      "Epoch: 24 cost time: 22.311497449874878\n",
      "Epoch: 24, Steps: 92 | Train Loss: 202.3627785 Vali Loss: 250.2193542 Test Loss: 183.0696930\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Updating learning rate to 1.1920928955078126e-11\n",
      "Epoch: 25 cost time: 22.280192613601685\n",
      "Epoch: 25, Steps: 92 | Train Loss: 206.8730013 Vali Loss: 245.8137817 Test Loss: 189.1783340\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Updating learning rate to 5.960464477539063e-12\n",
      "Epoch: 26 cost time: 21.758567094802856\n",
      "Epoch: 26, Steps: 92 | Train Loss: 211.1751816 Vali Loss: 245.5888000 Test Loss: 183.9189621\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Updating learning rate to 2.9802322387695314e-12\n",
      "Epoch: 27 cost time: 21.763175010681152\n",
      "Epoch: 27, Steps: 92 | Train Loss: 206.9329249 Vali Loss: 246.5631195 Test Loss: 187.1078094\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Updating learning rate to 1.4901161193847657e-12\n",
      "Epoch: 28 cost time: 21.57783842086792\n",
      "Epoch: 28, Steps: 92 | Train Loss: 201.2047551 Vali Loss: 244.0963043 Test Loss: 184.6307800\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Updating learning rate to 7.450580596923828e-13\n",
      "Epoch: 29 cost time: 21.854315280914307\n",
      "Epoch: 29, Steps: 92 | Train Loss: 202.7411847 Vali Loss: 246.4187775 Test Loss: 182.8344910\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Updating learning rate to 3.725290298461914e-13\n",
      "Epoch: 30 cost time: 21.974207878112793\n",
      "Epoch: 30, Steps: 92 | Train Loss: 204.9366958 Vali Loss: 247.9423950 Test Loss: 188.6305588\n",
      "EarlyStopping counter: 26 out of 100\n",
      "Updating learning rate to 1.862645149230957e-13\n",
      "Epoch: 31 cost time: 22.275529146194458\n",
      "Epoch: 31, Steps: 92 | Train Loss: 202.7397896 Vali Loss: 248.8831818 Test Loss: 182.7023956\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Updating learning rate to 9.313225746154786e-14\n",
      "Epoch: 32 cost time: 21.587984561920166\n",
      "Epoch: 32, Steps: 92 | Train Loss: 204.3675395 Vali Loss: 240.7411407 Test Loss: 190.4232727\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Updating learning rate to 4.656612873077393e-14\n",
      "Epoch: 33 cost time: 22.16893982887268\n",
      "Epoch: 33, Steps: 92 | Train Loss: 201.2877462 Vali Loss: 249.3310425 Test Loss: 187.5978226\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Updating learning rate to 2.3283064365386964e-14\n",
      "Epoch: 34 cost time: 21.67092514038086\n",
      "Epoch: 34, Steps: 92 | Train Loss: 206.7918674 Vali Loss: 242.5221710 Test Loss: 185.7589050\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Updating learning rate to 1.1641532182693482e-14\n",
      "Epoch: 35 cost time: 20.462838888168335\n",
      "Epoch: 35, Steps: 92 | Train Loss: 201.5000045 Vali Loss: 248.9873322 Test Loss: 190.7714462\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Updating learning rate to 5.820766091346741e-15\n",
      "Epoch: 36 cost time: 20.270756721496582\n",
      "Epoch: 36, Steps: 92 | Train Loss: 205.4091402 Vali Loss: 251.8395782 Test Loss: 192.3029922\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Updating learning rate to 2.9103830456733705e-15\n",
      "Epoch: 37 cost time: 19.758651733398438\n",
      "Epoch: 37, Steps: 92 | Train Loss: 205.7853854 Vali Loss: 252.9307648 Test Loss: 189.4809998\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Updating learning rate to 1.4551915228366853e-15\n",
      "Epoch: 38 cost time: 20.490261793136597\n",
      "Epoch: 38, Steps: 92 | Train Loss: 203.9051633 Vali Loss: 246.6032196 Test Loss: 187.6121262\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Updating learning rate to 7.275957614183426e-16\n",
      "Epoch: 39 cost time: 20.713095903396606\n",
      "Epoch: 39, Steps: 92 | Train Loss: 204.9612742 Vali Loss: 237.4659760 Test Loss: 189.4147003\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Updating learning rate to 3.637978807091713e-16\n",
      "Epoch: 40 cost time: 20.34276533126831\n",
      "Epoch: 40, Steps: 92 | Train Loss: 207.8382975 Vali Loss: 250.0642975 Test Loss: 185.1160706\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Updating learning rate to 1.8189894035458566e-16\n",
      "Epoch: 41 cost time: 20.154733896255493\n",
      "Epoch: 41, Steps: 92 | Train Loss: 207.7083772 Vali Loss: 251.4922882 Test Loss: 185.3738510\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Updating learning rate to 9.094947017729283e-17\n",
      "Epoch: 42 cost time: 19.880211114883423\n",
      "Epoch: 42, Steps: 92 | Train Loss: 204.3518239 Vali Loss: 250.2169128 Test Loss: 186.8054901\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Updating learning rate to 4.5474735088646414e-17\n",
      "Epoch: 43 cost time: 20.459379196166992\n",
      "Epoch: 43, Steps: 92 | Train Loss: 199.9459868 Vali Loss: 248.9901642 Test Loss: 184.4769531\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Updating learning rate to 2.2737367544323207e-17\n",
      "Epoch: 44 cost time: 19.663294315338135\n",
      "Epoch: 44, Steps: 92 | Train Loss: 203.9738187 Vali Loss: 248.4027313 Test Loss: 190.0455185\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Updating learning rate to 1.1368683772161604e-17\n",
      "Epoch: 45 cost time: 19.868293285369873\n",
      "Epoch: 45, Steps: 92 | Train Loss: 200.5457254 Vali Loss: 248.2399109 Test Loss: 181.6877548\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Updating learning rate to 5.684341886080802e-18\n",
      "Epoch: 46 cost time: 20.356415510177612\n",
      "Epoch: 46, Steps: 92 | Train Loss: 207.2929011 Vali Loss: 237.4946564 Test Loss: 185.4785156\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Updating learning rate to 2.842170943040401e-18\n",
      "Epoch: 47 cost time: 22.456881046295166\n",
      "Epoch: 47, Steps: 92 | Train Loss: 202.5387662 Vali Loss: 250.4521912 Test Loss: 187.7724335\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Updating learning rate to 1.4210854715202004e-18\n",
      "Epoch: 48 cost time: 19.75328302383423\n",
      "Epoch: 48, Steps: 92 | Train Loss: 202.1853641 Vali Loss: 248.6656433 Test Loss: 190.6047333\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Updating learning rate to 7.105427357601002e-19\n",
      "Epoch: 49 cost time: 19.060068130493164\n",
      "Epoch: 49, Steps: 92 | Train Loss: 200.1289037 Vali Loss: 236.9440491 Test Loss: 191.6795654\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Updating learning rate to 3.552713678800501e-19\n",
      "Epoch: 50 cost time: 19.774247646331787\n",
      "Epoch: 50, Steps: 92 | Train Loss: 205.4050061 Vali Loss: 252.0307800 Test Loss: 184.7263748\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Updating learning rate to 1.7763568394002505e-19\n",
      "Epoch: 51 cost time: 20.658124208450317\n",
      "Epoch: 51, Steps: 92 | Train Loss: 202.6758799 Vali Loss: 235.9734192 Test Loss: 185.9269943\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Updating learning rate to 8.881784197001253e-20\n",
      "Epoch: 52 cost time: 20.552582502365112\n",
      "Epoch: 52, Steps: 92 | Train Loss: 204.5036796 Vali Loss: 241.8110626 Test Loss: 181.5940765\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Updating learning rate to 4.4408920985006264e-20\n",
      "Epoch: 53 cost time: 19.5600643157959\n",
      "Epoch: 53, Steps: 92 | Train Loss: 201.8247322 Vali Loss: 248.6922913 Test Loss: 189.2966675\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Updating learning rate to 2.2204460492503132e-20\n",
      "Epoch: 54 cost time: 19.658694744110107\n",
      "Epoch: 54, Steps: 92 | Train Loss: 200.6247967 Vali Loss: 247.6378510 Test Loss: 185.6699097\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Updating learning rate to 1.1102230246251566e-20\n",
      "Epoch: 55 cost time: 20.67273449897766\n",
      "Epoch: 55, Steps: 92 | Train Loss: 206.4149337 Vali Loss: 229.4301392 Test Loss: 183.7828278\n",
      "Validation loss decreased (232.737869 --> 229.430139).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-21\n",
      "Epoch: 56 cost time: 20.292609691619873\n",
      "Epoch: 56, Steps: 92 | Train Loss: 209.4614531 Vali Loss: 244.0036560 Test Loss: 185.7605072\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 2.7755575615628915e-21\n",
      "Epoch: 57 cost time: 20.76307487487793\n",
      "Epoch: 57, Steps: 92 | Train Loss: 206.5979264 Vali Loss: 238.0429138 Test Loss: 188.3384857\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 1.3877787807814457e-21\n",
      "Epoch: 58 cost time: 19.370692253112793\n",
      "Epoch: 58, Steps: 92 | Train Loss: 203.4127965 Vali Loss: 233.6418152 Test Loss: 184.1169479\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 6.938893903907229e-22\n",
      "Epoch: 59 cost time: 19.625451803207397\n",
      "Epoch: 59, Steps: 92 | Train Loss: 198.1343878 Vali Loss: 242.0770782 Test Loss: 185.0388626\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 3.4694469519536144e-22\n",
      "Epoch: 60 cost time: 19.840275764465332\n",
      "Epoch: 60, Steps: 92 | Train Loss: 203.8281068 Vali Loss: 247.0538269 Test Loss: 191.0153763\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 1.7347234759768072e-22\n",
      "Epoch: 61 cost time: 20.66983962059021\n",
      "Epoch: 61, Steps: 92 | Train Loss: 205.5066763 Vali Loss: 251.7628967 Test Loss: 187.2187912\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 8.673617379884036e-23\n",
      "Epoch: 62 cost time: 20.36638116836548\n",
      "Epoch: 62, Steps: 92 | Train Loss: 205.4622584 Vali Loss: 252.2432495 Test Loss: 185.9994446\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 4.336808689942018e-23\n",
      "Epoch: 63 cost time: 19.87713384628296\n",
      "Epoch: 63, Steps: 92 | Train Loss: 203.1156184 Vali Loss: 248.5096252 Test Loss: 187.9537338\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 2.168404344971009e-23\n",
      "Epoch: 64 cost time: 19.989883184432983\n",
      "Epoch: 64, Steps: 92 | Train Loss: 201.4000130 Vali Loss: 248.7606827 Test Loss: 184.2780411\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 1.0842021724855045e-23\n",
      "Epoch: 65 cost time: 19.6724853515625\n",
      "Epoch: 65, Steps: 92 | Train Loss: 205.7274223 Vali Loss: 238.5528778 Test Loss: 188.9180481\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 5.4210108624275224e-24\n",
      "Epoch: 66 cost time: 19.353012800216675\n",
      "Epoch: 66, Steps: 92 | Train Loss: 204.5630361 Vali Loss: 242.8089539 Test Loss: 182.6700119\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 2.7105054312137612e-24\n",
      "Epoch: 67 cost time: 19.56854248046875\n",
      "Epoch: 67, Steps: 92 | Train Loss: 203.7020114 Vali Loss: 233.0821808 Test Loss: 186.1020020\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 1.3552527156068806e-24\n",
      "Epoch: 68 cost time: 18.889560222625732\n",
      "Epoch: 68, Steps: 92 | Train Loss: 201.4224867 Vali Loss: 249.2572113 Test Loss: 185.8277802\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Updating learning rate to 6.776263578034403e-25\n",
      "Epoch: 69 cost time: 19.06702423095703\n",
      "Epoch: 69, Steps: 92 | Train Loss: 200.8398844 Vali Loss: 250.0031708 Test Loss: 181.8065720\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Updating learning rate to 3.3881317890172015e-25\n",
      "Epoch: 70 cost time: 19.341258764266968\n",
      "Epoch: 70, Steps: 92 | Train Loss: 201.3966047 Vali Loss: 251.9051880 Test Loss: 185.7304398\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Updating learning rate to 1.6940658945086008e-25\n",
      "Epoch: 71 cost time: 19.34690761566162\n",
      "Epoch: 71, Steps: 92 | Train Loss: 203.2844774 Vali Loss: 229.1881317 Test Loss: 193.4278061\n",
      "Validation loss decreased (229.430139 --> 229.188132).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-26\n",
      "Epoch: 72 cost time: 19.06266736984253\n",
      "Epoch: 72, Steps: 92 | Train Loss: 203.7958916 Vali Loss: 244.0111115 Test Loss: 188.1670120\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 4.235164736271502e-26\n",
      "Epoch: 73 cost time: 19.175893545150757\n",
      "Epoch: 73, Steps: 92 | Train Loss: 202.3987096 Vali Loss: 245.5711990 Test Loss: 185.1917221\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 2.117582368135751e-26\n",
      "Epoch: 74 cost time: 19.04063868522644\n",
      "Epoch: 74, Steps: 92 | Train Loss: 201.4946834 Vali Loss: 249.9154266 Test Loss: 186.5771149\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 1.0587911840678755e-26\n",
      "Epoch: 75 cost time: 18.95261549949646\n",
      "Epoch: 75, Steps: 92 | Train Loss: 204.1251683 Vali Loss: 242.6143524 Test Loss: 179.3935364\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 5.2939559203393774e-27\n",
      "Epoch: 76 cost time: 18.969854831695557\n",
      "Epoch: 76, Steps: 92 | Train Loss: 202.4147011 Vali Loss: 228.8826324 Test Loss: 185.2822220\n",
      "Validation loss decreased (229.188132 --> 228.882632).  Saving model ...\n",
      "Updating learning rate to 2.6469779601696887e-27\n",
      "Epoch: 77 cost time: 19.571979761123657\n",
      "Epoch: 77, Steps: 92 | Train Loss: 203.0594945 Vali Loss: 245.9432861 Test Loss: 184.4526642\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 1.3234889800848443e-27\n",
      "Epoch: 78 cost time: 19.26011824607849\n",
      "Epoch: 78, Steps: 92 | Train Loss: 203.9898655 Vali Loss: 243.3507324 Test Loss: 188.3394089\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 6.617444900424222e-28\n",
      "Epoch: 79 cost time: 18.863642930984497\n",
      "Epoch: 79, Steps: 92 | Train Loss: 200.4978070 Vali Loss: 251.2365265 Test Loss: 186.0137741\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 3.308722450212111e-28\n",
      "Epoch: 80 cost time: 19.377309322357178\n",
      "Epoch: 80, Steps: 92 | Train Loss: 205.5780745 Vali Loss: 243.0658722 Test Loss: 186.1728989\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 1.6543612251060554e-28\n",
      "Epoch: 81 cost time: 19.659177541732788\n",
      "Epoch: 81, Steps: 92 | Train Loss: 206.3066440 Vali Loss: 251.6642059 Test Loss: 191.3720963\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 8.271806125530277e-29\n",
      "Epoch: 82 cost time: 18.957866668701172\n",
      "Epoch: 82, Steps: 92 | Train Loss: 204.0607711 Vali Loss: 251.1274750 Test Loss: 188.7744659\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 4.1359030627651386e-29\n",
      "Epoch: 83 cost time: 19.45632576942444\n",
      "Epoch: 83, Steps: 92 | Train Loss: 204.4533180 Vali Loss: 242.0550369 Test Loss: 187.6917923\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 2.0679515313825693e-29\n",
      "Epoch: 84 cost time: 19.28988218307495\n",
      "Epoch: 84, Steps: 92 | Train Loss: 203.1318577 Vali Loss: 250.2394287 Test Loss: 185.6198944\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 1.0339757656912846e-29\n",
      "Epoch: 85 cost time: 19.34771418571472\n",
      "Epoch: 85, Steps: 92 | Train Loss: 205.7168985 Vali Loss: 249.9802124 Test Loss: 191.2947876\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 5.169878828456423e-30\n",
      "Epoch: 86 cost time: 19.111947774887085\n",
      "Epoch: 86, Steps: 92 | Train Loss: 206.0132910 Vali Loss: 251.9856079 Test Loss: 186.4420609\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 2.5849394142282116e-30\n",
      "Epoch: 87 cost time: 19.57111096382141\n",
      "Epoch: 87, Steps: 92 | Train Loss: 202.9498885 Vali Loss: 235.6040710 Test Loss: 185.0805328\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 1.2924697071141058e-30\n",
      "Epoch: 88 cost time: 18.667354583740234\n",
      "Epoch: 88, Steps: 92 | Train Loss: 202.8403902 Vali Loss: 241.7631226 Test Loss: 187.4017410\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Updating learning rate to 6.462348535570529e-31\n",
      "Epoch: 89 cost time: 19.15746235847473\n",
      "Epoch: 89, Steps: 92 | Train Loss: 200.6570508 Vali Loss: 226.3813995 Test Loss: 186.0187271\n",
      "Validation loss decreased (228.882632 --> 226.381400).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852645e-31\n",
      "Epoch: 90 cost time: 19.13494062423706\n",
      "Epoch: 90, Steps: 92 | Train Loss: 204.8250639 Vali Loss: 235.9945984 Test Loss: 187.9089951\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Updating learning rate to 1.6155871338926323e-31\n",
      "Epoch: 91 cost time: 19.366060972213745\n",
      "Epoch: 91, Steps: 92 | Train Loss: 205.6899419 Vali Loss: 244.5825897 Test Loss: 183.6055893\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Updating learning rate to 8.077935669463161e-32\n",
      "Epoch: 92 cost time: 19.39238476753235\n",
      "Epoch: 92, Steps: 92 | Train Loss: 199.9146111 Vali Loss: 249.7129288 Test Loss: 188.6058838\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Updating learning rate to 4.0389678347315806e-32\n",
      "Epoch: 93 cost time: 19.060840368270874\n",
      "Epoch: 93, Steps: 92 | Train Loss: 203.2228314 Vali Loss: 250.5334900 Test Loss: 187.6960922\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Updating learning rate to 2.0194839173657903e-32\n",
      "Epoch: 94 cost time: 19.64906883239746\n",
      "Epoch: 94, Steps: 92 | Train Loss: 204.1742580 Vali Loss: 248.5906708 Test Loss: 190.8829208\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Updating learning rate to 1.0097419586828952e-32\n",
      "Epoch: 95 cost time: 19.567675828933716\n",
      "Epoch: 95, Steps: 92 | Train Loss: 209.9473078 Vali Loss: 241.1353699 Test Loss: 187.5152313\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Updating learning rate to 5.048709793414476e-33\n",
      "Epoch: 96 cost time: 19.27396035194397\n",
      "Epoch: 96, Steps: 92 | Train Loss: 203.4050576 Vali Loss: 250.0740692 Test Loss: 183.5343857\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Updating learning rate to 2.524354896707238e-33\n",
      "Epoch: 97 cost time: 19.16786241531372\n",
      "Epoch: 97, Steps: 92 | Train Loss: 200.2991117 Vali Loss: 247.4943573 Test Loss: 180.6619263\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Updating learning rate to 1.262177448353619e-33\n",
      "Epoch: 98 cost time: 19.47281789779663\n",
      "Epoch: 98, Steps: 92 | Train Loss: 204.1139338 Vali Loss: 242.7949402 Test Loss: 190.4696259\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Updating learning rate to 6.310887241768095e-34\n",
      "Epoch: 99 cost time: 18.579659700393677\n",
      "Epoch: 99, Steps: 92 | Train Loss: 204.6915045 Vali Loss: 250.6461731 Test Loss: 187.0140671\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Updating learning rate to 3.1554436208840474e-34\n",
      "Epoch: 100 cost time: 19.051631927490234\n",
      "Epoch: 100, Steps: 92 | Train Loss: 202.2113841 Vali Loss: 250.3965637 Test Loss: 191.0547729\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Updating learning rate to 1.5777218104420237e-34\n",
      ">>>>>>>testing : informer_custom_ftMS_sl45_ll3_pl1_dm1078_nh11_el4_dl2_df2048_atprob_fc4_ebtimeF_dtTrue_mxTrue_exp_0relu2021_2023_1HE100P3<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 391\n",
      "test shape: (5, 76, 1, 1) (5, 76, 1, 1)\n",
      "test shape: (380, 1, 1) (380, 1, 1)\n",
      "mse:186.0074462890625, mae:10.29833698272705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MAE</td><td>▁</td></tr><tr><td>MAE_Vali</td><td>█▇▂▃▂▂▃▂▂▂▂▂▂▂▃▂▃▂▁▂▁▂▂▂▃▂▁▂▂▃▂▃▃▂▁▁▂▂▂▂</td></tr><tr><td>MAPE</td><td>▁</td></tr><tr><td>MAPE_Vali</td><td>██▂▃▂▂▃▂▂▃▂▁▂▂▁▃▁▃▂▃▁▂▃▂▃▂▂▂▃▂▂▃▃▃▂▂▃▃▂▃</td></tr><tr><td>MSE</td><td>▁</td></tr><tr><td>MSE_Vali</td><td>█▅▃▄▃▃▄▄▃▄▃▃▄▃▄▂▄▄▂▂▂▃▃▃▄▄▂▄▃▄▃▄▄▄▂▁▄▄▃▄</td></tr><tr><td>MSPE</td><td>▁</td></tr><tr><td>MSPE_Vali</td><td>█▆▃▃▃▃▃▃▂▃▃▂▃▃▁▃▁▃▃▃▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>RMSE</td><td>▁</td></tr><tr><td>RMSE_Vali</td><td>█▅▃▄▃▃▄▄▄▄▃▃▄▃▄▂▄▄▂▂▂▄▃▃▄▄▂▄▃▄▃▄▄▄▂▁▄▄▃▄</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_loss</td><td>█▁▃▅▄▃▄▅▃▃▃▃▃▄▅▅▄▅▄▅▄▄▄▃▄▃▄▃▄▄▃▄▄▄▃▄▄▅▂▅</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>vali_loss</td><td>█▅▃▄▃▃▄▄▃▄▃▃▄▃▄▂▄▄▂▂▂▃▃▃▄▄▂▄▃▄▃▄▄▄▂▁▄▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MAE</td><td>10.29834</td></tr><tr><td>MAE_Vali</td><td>10.65829</td></tr><tr><td>MAPE</td><td>0.11471</td></tr><tr><td>MAPE_Vali</td><td>0.25825</td></tr><tr><td>MSE</td><td>186.00745</td></tr><tr><td>MSE_Vali</td><td>250.39656</td></tr><tr><td>MSPE</td><td>0.02081</td></tr><tr><td>MSPE_Vali</td><td>0.40789</td></tr><tr><td>RMSE</td><td>13.63845</td></tr><tr><td>RMSE_Vali</td><td>15.82392</td></tr><tr><td>epoch</td><td>99</td></tr><tr><td>test_loss</td><td>191.05477</td></tr><tr><td>train_loss</td><td>202.21138</td></tr><tr><td>vali_loss</td><td>250.39656</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-paper-2502</strong> at: <a href='https://wandb.ai/ossyandlars/ELprices/runs/dlw7nj0c' target=\"_blank\">https://wandb.ai/ossyandlars/ELprices/runs/dlw7nj0c</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231207_164748-dlw7nj0c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "run =wandb.init(project='Elprices', entity='ossyandlars')\n",
    "wandb.config.update(args)\n",
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}relu2021_2023_1HE100P3'.format(args.model, args.data, args.features,\n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = Exp(args)\n",
    "\n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "\n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "    if args.do_predict:\n",
    "        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.predict(setting, True)\n",
    "        \n",
    "    run.finish()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CDHF-HerAE3u"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0YI1zx6ACiz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set saved model path\n",
    "setting = 'informer_custom_ftMS_sl720_ll48_pl24_dm1360_nh14_el4_dl1_df2048_atprob_fc7_ebtimeF_dtTrue_mxTrue_exp_0relu38'\n",
    "#path = os.path.join(args.checkpoints,setting,'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTkluNNcyMJt",
    "outputId": "780767fe-6321-4081-e827-6701daeb375b"
   },
   "outputs": [],
   "source": [
    "# If you already have a trained model, you can set the arguments and model path, then initialize a Experiment and use it to predict\n",
    "# Prediction is a sequence which is adjacent to the last date of the data, and does not exist in the data\n",
    "# If you want to get more information about prediction, you can refer to code `exp/exp_informer.py function predict()` and `data/data_loader.py class Dataset_Pred`\n",
    "\n",
    "exp = Exp(args)\n",
    "\n",
    "exp.predict(setting, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBCPbjGuzAZb",
    "outputId": "945dc447-88e8-4b08-b7e5-f0a0b486d138"
   },
   "outputs": [],
   "source": [
    "# the prediction will be saved in ./results/{setting}/real_prediction.npy\n",
    "import numpy as np\n",
    "\n",
    "prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yFuVkTV30_j"
   },
   "source": [
    "### More details about Prediction - prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sv9AR_Aw030r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('./results/'+setting+'/predictions_and_ground_truths.csv')  # Replace with your file path\n",
    "\n",
    "# Ensure only numeric columns are used for plotting\n",
    "numeric_data = data.select_dtypes(include='number')\n",
    "\n",
    "# Selecting 6 random rows from the numeric data\n",
    "random_rows_numeric = numeric_data.sample(n=6)\n",
    "\n",
    "# Plotting the first 24 columns against the second 24 columns for these rows\n",
    "fig, axs = plt.subplots(6, figsize=(12, 18))\n",
    "\n",
    "for i, (idx, row) in enumerate(random_rows_numeric.iterrows()):\n",
    "    # Splitting the row into two parts\n",
    "    first_24 = row.iloc[:24]\n",
    "    second_24 = row.iloc[24:48]\n",
    "\n",
    "    # Plotting\n",
    "    axs[i].plot(first_24.values, label='First 24 Columns', marker='o')\n",
    "    axs[i].plot(second_24.values, label='Second 24 Columns', marker='x')\n",
    "    axs[i].set_title(f'Row {idx}')\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVLWZL2a1pwB",
    "outputId": "421e9ae1-f024-42b6-c8cb-ed1d38c864cd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "NwtZmQC71uc8",
    "outputId": "eec9d116-f122-42d9-8e02-c893ff764db0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnePVyrW4I14"
   },
   "source": [
    "### More details about Prediction - prediction dataset\n",
    "\n",
    "You can give a `root_path` and `data_path` of the data you want to forecast, and set `seq_len`, `label_len`, `pred_len` and other arguments as other Dataset. The difference is that you can set a more detailed freq such as `15min` or `3h` to generate the timestamp of prediction series.\n",
    "\n",
    "`Dataset_Pred` only has one sample (including `encoder_input: [1, seq_len, dim]`, `decoder_token: [1, label_len, dim]`, `encoder_input_timestamp: [1, seq_len, date_dim]`, `decoder_input_timstamp: [1, label_len+pred_len, date_dim]`). It will intercept the last sequence of the given data (seq_len data) to forecast the unseen future sequence (pred_len data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZpXhNGp34Hf4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4Rpd1q74T8N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "42C84BfY6UPV",
    "outputId": "f5ccc428-db92-4708-e104-f5d29aa5adf9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNhEP_7sAgqC"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMRk8VkQ2Iko",
    "outputId": "bbf3cd10-7294-472d-e330-21e00f20963a"
   },
   "outputs": [],
   "source": [
    "# When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "# The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "preds = np.load('./results/'+setting+'/pred.npy')\n",
    "trues = np.load('./results/'+setting+'/true.npy')\n",
    "metrics = np.load('./results/'+setting+'/metrics.npy')\n",
    "#real = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_csv(setting):\n",
    "    # Load the arrays\n",
    "    preds = np.load('./results/'+setting+'/pred.npy')\n",
    "    trues = np.load('./results/'+setting+'/true.npy')\n",
    "\n",
    "    # Reshape the arrays into 2D format\n",
    "    # Assuming preds and trues have the same shape\n",
    "    num_samples, pred_len, dimensions = preds.shape\n",
    "    preds_reshaped = preds.reshape(num_samples, pred_len * dimensions)\n",
    "    trues_reshaped = trues.reshape(num_samples, pred_len * dimensions)\n",
    "\n",
    "    # Concatenate preds and trues for each sample\n",
    "    combined = np.concatenate((preds_reshaped, trues_reshaped), axis=1)\n",
    "\n",
    "    # Create a DataFrame and save to CSV\n",
    "    df = pd.DataFrame(combined)\n",
    "    df.to_csv(f'./results/{setting}/predictions_and_ground_truths.csv', index=False)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "save_to_csv(setting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEGhDOmxAeAb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "mae = metrics[0]\n",
    "mse = metrics[1]\n",
    "rmse = metrics[2]\n",
    "mape = metrics[3]\n",
    "mspe = metrics[4]\n",
    "\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAPE: {mape}')\n",
    "print(f'MSPE: {mspe}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "kyPuOPGAAjl3",
    "outputId": "8554f6f8-c13a-43e1-b04b-5f27823445d0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(preds[:, :, -1], label='Predicted', color='blue')\n",
    "plt.plot(trues[:, :, -1], label='True', color='green')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Predicted vs True Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "43MIgWfpMYIB",
    "outputId": "327f64b7-363c-44f9-c7c8-1f654911068c"
   },
   "outputs": [],
   "source": [
    "# draw HUFL prediction\n",
    "plt.figure()\n",
    "plt.plot(trues[:,:,0], label='GroundTruth')\n",
    "plt.plot(preds[:,:,0], label='Prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKmqhCfmt0xd"
   },
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_Daily\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "Data = Dataset_ETT_hour\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    freq=args.freq\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iflTTl0quCoK",
    "outputId": "3708fc91-517e-4c83-e133-059381bde271"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "args.output_attention = True\n",
    "\n",
    "exp = Exp(args)\n",
    "\n",
    "model = exp.model\n",
    "\n",
    "setting = 'informer_Data_ftMS_sl60_ll15_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0'\n",
    "path = os.path.join(args.checkpoints,setting,'checkpoint.pth')\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDdzqm9HAk2C"
   },
   "outputs": [],
   "source": [
    "# attention visualization\n",
    "idx = 0\n",
    "for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(data_loader):\n",
    "    if i!=idx:\n",
    "        continue\n",
    "    batch_x = batch_x.float().to(exp.device)\n",
    "    batch_y = batch_y.float()\n",
    "\n",
    "    batch_x_mark = batch_x_mark.float().to(exp.device)\n",
    "    batch_y_mark = batch_y_mark.float().to(exp.device)\n",
    "\n",
    "    dec_inp = torch.zeros_like(batch_y[:,-args.pred_len:,:]).float()\n",
    "    dec_inp = torch.cat([batch_y[:,:args.label_len,:], dec_inp], dim=1).float().to(exp.device)\n",
    "\n",
    "    outputs,attn = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWef23vWAmUz",
    "outputId": "021eca83-e12f-402c-c87e-4fffa643d2f1"
   },
   "outputs": [],
   "source": [
    "attn[0].shape, attn[1].shape #, attn[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iZDH1fZgAnrl",
    "outputId": "991cae95-04a2-402d-f179-777e962f46fe"
   },
   "outputs": [],
   "source": [
    "layer = 0\n",
    "distil = 'Distil' if args.distil else 'NoDistil'\n",
    "for h in range(0,8):\n",
    "    plt.figure(figsize=[10,8])\n",
    "    plt.title('Informer, {}, attn:{} layer:{} head:{}'.format(distil, args.attn, layer, h))\n",
    "    A = attn[layer][0,h].detach().cpu().numpy()\n",
    "    ax = sns.heatmap(A, vmin=0, vmax=A.max()+0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DQFGYE3KAozo",
    "outputId": "10489845-4e23-4b55-8509-edf50d77394d"
   },
   "outputs": [],
   "source": [
    "layer = 1\n",
    "distil = 'Distil' if args.distil else 'NoDistil'\n",
    "for h in range(0,8):\n",
    "    plt.figure(figsize=[10,8])\n",
    "    plt.title('Informer, {}, attn:{} layer:{} head:{}'.format(distil, args.attn, layer, h))\n",
    "    A = attn[layer][0,h].detach().cpu().numpy()\n",
    "    ax = sns.heatmap(A, vmin=0, vmax=A.max()+0.01)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ew9vekI9Mw8f"
   },
   "source": [
    "## Custom Data\n",
    "\n",
    "Custom data (xxx.csv) has to include at least 2 features: `date`(format: `YYYY-MM-DD hh:mm:ss`) and `target feature`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqQBJWHeMzP-"
   },
   "outputs": [],
   "source": [
    "from data.data_loader import Dataset_Custom\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bFrfuw6Oxpi"
   },
   "outputs": [],
   "source": [
    "# custom data: xxx.csv\n",
    "# data features: ['date', ...(other features), target feature]\n",
    "\n",
    "# we take ETTh2 as an example\n",
    "args.root_path = './ETDataset/ETT-small/'\n",
    "args.data_path = 'ETTh2.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(args.root_path, args.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "0--9JC0eO_WT",
    "outputId": "c8b9c2a3-0400-44d4-8a4e-85e42194ea59"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmvRipRbPAbP"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We set 'HULL' as target instead of 'OT'\n",
    "\n",
    "The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "'''\n",
    "\n",
    "args.target = 'HULL'\n",
    "args.freq = 'h'\n",
    "\n",
    "Data = Dataset_Custom\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    target=args.target, # HULL here\n",
    "    freq=args.freq # 'h': hourly, 't':minutely\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkNDT2jMPCUf"
   },
   "outputs": [],
   "source": [
    "batch_x,batch_y,batch_x_mark,batch_y_mark = data_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VUcvSLlkSFTx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
